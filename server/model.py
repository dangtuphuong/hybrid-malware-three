import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Reshape, Bidirectional
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import math
import io
from datetime import datetime
import pickle

def train_model(data, label, connection, user):

    # Load the dataset
    print("Drop SHA256")
    X = data.drop(columns=[label])
    y = data[label]

    nFeatures = X.shape[1]
    reshapeD = math.ceil(math.sqrt(nFeatures))
    numLabel = y.nunique()

    print("Reshape the data")
    # Fill missing values (if any) with the mean of each column
    X.fillna(X.mean(), inplace=True)

    # Normalize the data using MinMaxScaler
    scaler = StandardScaler()
    label_encoder = LabelEncoder()
    X_scaled = scaler.fit_transform(X)
    y_encoded = label_encoder.fit_transform(y)

    # Reshape the data into a grid (let's assume a 10x10 grid as an example)
    # If there are fewer than 100 features, we will pad the data
    X_reshaped = np.zeros((X_scaled.shape[0], reshapeD, reshapeD, 1))  # Extra dimension for CNN channels
    for i in range(X_scaled.shape[0]):
        for j in range(reshapeD * reshapeD):
            # Fill the first N values into the grid (reshape into 10x10)
            if (j < nFeatures):
                X_reshaped[i, j // reshapeD, j % reshapeD, 0] = X_scaled[i][j]
            else:
                X_reshaped[i, j // reshapeD, j % reshapeD, 0] = 0;

    print("Finish reshape")

    # Split the data into train and test sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_encoded, test_size=0.1, random_state=42)

    y_train_cat = to_categorical(y_train, numLabel)
    y_test_cat = to_categorical(y_test, numLabel)

    print("Building CNN model")
    # Build a CNN model
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(reshapeD, reshapeD, 1)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    # Flatten the output of CNN and reshape to prepare for LSTM
    model.add(Flatten())

    input_shape = math.floor((math.floor((reshapeD - 4) / 2) - 3) / 2) + 1

    # Reshape to (sequence_length, num_features) for LSTM
    # After flattening, we reshape it to have sequence length and features (e.g., 5 timesteps, 128 features)
    model.add(Reshape((input_shape, 64)))

    # LSTM Layer to capture sequential dependencies
    model.add(Bidirectional(LSTM(64)))

    model.add(Dense(64, activation='relu'))

    model.add(Dense(numLabel, activation='softmax'))  

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)

    # Train the model
    history = model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test, y_test_cat), callbacks=[early_stopping, lr_scheduler])

    # Evaluate the model
    test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)

    print(f"Test Accuracy: {test_accuracy}")

    #Save model to db
    cursor = connection.cursor()

    description = describe_data(data)

    insert_model_q = "INSERT INTO models (id, modelName, modelBlob, createdBy, featureDis, description) VALUES (%s, %s, %s, %s, %s, %s)"
    mode_name = 'CNN + LSTM'
    model_id = int(datetime.now().strftime("%Y%m%d%H%M%S"))
    model_data = {
        'model': model,
        'label_encoder': label_encoder
    }
    model_blob = pickle.dumps(model_data)
    cursor.execute(insert_model_q, (model_id, mode_name, model_blob, user, y.value_counts().to_json(), description))

    connection.commit()

    training_history = history.history

    insert_history_model_q = "INSERT INTO model_history (epoch, loss, accuracy, valLoss, valAccuracy, modelID) VALUES (%s, %s, %s, %s, %s, %s)"

    epoches = []
    for epoch, (loss, acc, val_loss, val_accuracy) in enumerate(zip(training_history['loss'], training_history['accuracy'], training_history['val_loss'], training_history['val_accuracy'])):
        cursor.execute(insert_history_model_q, (epoch, loss, acc, val_loss, val_accuracy, model_id))
        epoches.append({
            "epoch": epoch,
            "loss": loss,
            "accuracy": acc,
            "val_loss": val_loss,
            "val_accuracy": val_accuracy
        })

    connection.commit()

    cursor.close()
    connection.close()

    return model_id, y.value_counts().to_dict(), epoches, description

def predict(data, label, model, encoder):
    X = data.drop(columns=[label])

    nFeatures = X.shape[1]
    reshapeD = math.ceil(math.sqrt(nFeatures))

    print("Reshape the data")
    # Fill missing values (if any) with the mean of each column
    X.fillna(X.mean(), inplace=True)

    # Normalize the data using MinMaxScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Reshape the data into a grid (let's assume a 10x10 grid as an example)
    # If there are fewer than 100 features, we will pad the data
    X_reshaped = np.zeros((X_scaled.shape[0], reshapeD, reshapeD, 1))  # Extra dimension for CNN channels
    for i in range(X_scaled.shape[0]):
        for j in range(reshapeD * reshapeD):
            # Fill the first N values into the grid (reshape into 10x10)
            if (j < nFeatures):
                X_reshaped[i, j // reshapeD, j % reshapeD, 0] = X_scaled[i][j]
            else:
                X_reshaped[i, j // reshapeD, j % reshapeD, 0] = 0;

    predictions = model.predict(X_reshaped)
    predicted_classes = np.argmax(predictions, axis=1)
    predicted_labels = encoder.inverse_transform(predicted_classes)

    return predicted_labels.tolist()

def analyse_data(data, label, model, connection):

    nData = data.drop(columns=[label])
    correlation_matrix = nData.corr()

    plt.figure(figsize=(15, 10))
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)
    plt.title('Feature Correlation Matrix')
    img = io.BytesIO()
    plt.savefig(img, format='png')
    img.seek(0)

    cursor = connection.cursor()
    insert_img_q = "UPDATE models SET corImg = %s WHERE id = %s"
    img_binary = img.read()
    cursor.execute(insert_img_q, (img_binary, int(model)))
    connection.commit()

    cursor.close()
    connection.close()

    return img

def describe_data(data):
    description = data.describe().T

    # Convert the summary statistics to JSON format
    description_json = description.to_json(orient='index')
    return description_json

def PCA_data(data, label, model, connection):
    nData = data.drop(columns=[label])
    correlation_matrix = nData.corr()

    # Get the absolute correlation values and sort
    sorted_corr = correlation_matrix.abs().unstack().sort_values(kind="quicksort", ascending=False)

    # Get top N feature correlations (excluding self-correlations)
    top_corr_pairs = sorted_corr[sorted_corr < 1].head(20)

    # Plot the top N correlations as a heatmap
    plt.clf()
    plt.figure(figsize=(10, 8))
    sns.heatmap(top_corr_pairs.unstack().abs(), annot=True, cmap='coolwarm')
    plt.title('Top 20 correlation features')
    img = io.BytesIO()
    plt.savefig(img, format='png')
    img.seek(0)

    cursor = connection.cursor()
    insert_img_q = "UPDATE models SET topCorImg = %s WHERE id = %s"
    img_binary = img.read()
    cursor.execute(insert_img_q, (img_binary, int(model)))
    connection.commit()

    cursor.close()
    connection.close()

    return img